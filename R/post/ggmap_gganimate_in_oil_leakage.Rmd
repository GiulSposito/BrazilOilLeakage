---
title: "Using ggmap and gganimation to visualize oil leakage in Brazil"
output:
  html_document:
    df_print: paged
---

Um derrame de petróleo cru que atingiu mais de 2 mil quilômetros do litoral das regiões Nordeste e Sudeste do Brasil, afetando mais de 400 praias em mais de 200 municípios diferentes. Os primeiros registros do derrame ocorreram no fim do mês de agosto de 2019 com avistamentos que ainda se alastram neste fim do ano. Este post expora os registros de avistamento disponíveis no site do IBAMA para visualizar o impacto do vazamento usando ggmap e gganimation.

<!--more--> 

Um derrame de petróleo cr que atingiu mais de 2 mil quilômetros do litoral das regiões Nordeste e Sudeste do Brasil, afetando mais de 400 praias em mais de 200 municípios diferentes. Os primeiros registros do derrame ocorreram no fim do mês de agosto de 2019 com avistamentos que ainda se alastram neste fim do ano. Mais de mil toneladas de óleo já foram recolhidas das praias, sendo este, portando, o pior vazamento de ólio na história do Brazil e o maior desastre ambiental já registrado na costa brasileira.

[derrame de petróleo cru](https://en.wikipedia.org/wiki/2019_Northeast_Brazil_oil_spill)

Neste post exploraremos os dados de avistamento de óleo disponíveis no site do IBAMA - Instituto Brasileiro do Meio Ambiente e dos Recursos Naturais Renovaveis, órgão do Ministério do Meio Ambiente que é órgão estatal responsável pela execução das políticas de preservação ambiental federal, para visualizar a evolução do vasamento usando os pacotes `ggmap` e `gganimation`.

[dados de avistamento de óleo](http://www.ibama.gov.br/manchasdeoleo)
[IBAMA - Instituto Brasileiro do Meio Ambiente e dos Recursos Naturais Renovaveis](http://www.ibama.gov.br)
[Ministério do Meio Ambiente](https://www.mma.gov.br/)

```{r setup, echo=FALSE}
# put rnotebook in the same workdir
knitr::opts_knit$set(root.dir = normalizePath(rprojroot::find_rstudio_root_file())) 
```

## Site and Dataset

O IBAMA disponibilizou informações específicas do vazamento de óleo em uma subsessão de seu site, mantendo um registro dos status e dos avistamentos de manchas atualizado diariamente.

[seu site](http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas)
[registro dos status e dos avistamentos](http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas)

Parte dos registros estão em formato do PDF, mas parte deles estão disponibilizados em formato excel, trazendo as seguintes informações: o nome de cada localidade em que já houve avistamento de manchas, município, data do primeiro avistamento, estado, latitude, longitude, data em que a localidade foi revisitada e status do local no momento.

## Data Scrapping

Embora o site disponbilize PDFs e XLXS, e seja possível explorar a tabela de avistamentos dentros do PDFs atraves do pacote `tabulizer`, neste post vamos apenas explorar o conteúdo dos arquivos em formato excel. O primeiro passo para tal é fazer o "scrapping" da página que disponibiliza os dados para extraír os links e baixar os arquivos.

```{r excelScrapping, eval=FALSE}
library(tidyverse)  # of course
library(rvest)      # to handle html scrapping
library(lubridate)  # handle datetime formats
library(glue)       # easily concat texts
library(knitr)      # markdown output tables
library(kableExtra) # formate markdown tables

# scrap and build url table
base_url <- "http://www.ibama.gov.br"

# page with update table
page <- read_html("http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas")

# get the table
updates <- page %>% 
  html_table() %>% 
  .[[1]]

# get the href links
doc_links <- page %>%
  html_nodes(xpath = "//td/a") %>% 
  html_attr("href")

# put then togheter
doc_list <- updates %>% 
  set_names(c("date","description","items")) %>% 
  mutate( type = str_extract(items, "^[\\w]+") ) %>% 
  mutate( type = ifelse(type=="XLS","XLSX", type)) %>% # one file with "xls" extension
  mutate( link = paste0(base_url, doc_links) ) %>% 
  mutate( date=dmy(date) ) %>%
  as_tibble()

# save it, just in case we want to recover later
saveRDS(doc_list,"./data/oil_leakage_doc_list.rds")

# let's see the links list
doc_list %>% 
  head(10) %>% 
  kable() %>% 
  kableExtra::kable_styling(font_size = 10)

```


```{r readDocList, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)  # of course
library(rvest)      # to handle html scrapping
library(lubridate)  # handle datetime formats
library(glue)       # easily concat texts
library(knitr)      # markdown output tables
library(kableExtra) # formate markdown tables

# reading from cache
doc_list <- readRDS("./data/oil_leakage_doc_list.rds")

# let's see the links list
doc_list %>% 
  head(10) %>% 
  kable() %>% 
  kableExtra::kable_styling(font_size = 10)
```

Agora que temos os links em mãos, vamos fazer o download dos arquivos XLSX.

```{r excelDownload, eval=FALSE, message=FALSE, warning=FALSE}
# check the pre existence of destination folders
XLSX_DEST_FOLDER <- "./data/oil_leakage_raw"
if(!file.exists("./data")) dir.create("./data")
if(!file.exists(XLSX_DEST_FOLDER)) dir.create(XLSX_DEST_FOLDER)

# download xlsx files
xlsx_filenames <- doc_list %>% 
  filter(type=="XLSX") %>% 
  select(date, link) %>% 
  arrange(date) %>% 
  split(1:nrow(.)) %>% 
  map_chr(function(.x){
    filename <- paste0(XLSX_DEST_FOLDER, "/",as.character(.x$date[1]), ".xlsx")
    download.file(url=.x$link[1], destfile = filename, mode = "wb")
    return(filename)
  })

# save an excel filename index
xlsx_index <- doc_list %>% 
  filter( type=="XLSX" ) %>% 
  arrange(date) %>% 
  mutate( filename = xlsx_filenames )

# save it
saveRDS(xlsx_index, "./data/excel_file_index.rds")

# let's see what we have
xlsx_index %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 9)
```

```{r readFileIndex, echo=FALSE}
xlsx_index <- readRDS("./data/excel_file_index.rds")

# let's see what we have
xlsx_index %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 9)
```

## Import Data

Vamos usar o pacote `xlsx` para ler os arquivos excels e importá-los num `data.frame`. Vamos dar uma olhada em um deles.

```{r viewXLSX}
library(xlsx) # import excel files (requires java/rjava)

xlsx_file <- read.xlsx(file = "./data/oil_leakage_raw/2019-12-14.xlsx",sheetIndex = 1)

xlsx_file %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 10)

```

Podemos ver que o arquivo contém: um _geocode_, o nome de cada localidade em que já houve avistamento de manchas, um _id_ para a localidade, o nome do município, nome do estado e sigla da unidade da federação, a data do primeiro avistamento, a data da revisão (última posição do status), a informação sobre o estado da localidade quanto ao vazamento, latitude e longitude, além de uma "hora" e de uma coluna "cou" desconhecida.

Para utilizar os arquivos, serãop necessários resolver o encoding, fatorizar o status, e transformar as notações de grau para latitude e longitude em notação decimal.

### Importation 

A importação dos arquivos é direta, primeiro vamos importar todos os arquivos e então fazer o tratamento e limpeza dos dados

```{r xlsxImport, eval=FALSE, message=FALSE, warning=FALSE}

# import each xlsx_file
files <- xlsx_index %>%
  split(1:nrow(.)) %>% 
  map(function(.x){
    print(.x$filename)
    read.xlsx(file = .x$filename[1], sheetIndex = 1, colClasses="character", stringsAsFactors=F) %>% 
      as_tibble() %>% 
      mutate(file.date=.x$date[1])
  })

# save it as cache
saveRDS(files, "./data/oil_leakage_imported_raw_excel.rds")

```

### Data Clean-up

```{r dataCleanup, message=FALSE, warning=FALSE}

# auxiliary function to translate the degree coordinates to decimal coordinates
translateCoord <- function(.coords){
  
  # extract orientation (N,S,W,E)
  directions <- str_extract(.coords,"\\w$")
  
  # split the numbers and rebuilds as xxDyy'zz"
  numbers <- .coords %>% 
    str_extract_all("(\\d\\.*)+") %>% 
    purrr::map(~paste0(.x[1],"d",.x[2],"'",.x[3],"\""))
  
  # modify char2dms to a safity call
  safe_char2dms <- safely(sp::char2dms, otherwise = NA)
  
  # readd orientation(NSWE) and converts to decimal
  numbers %>% 
    purrr::map2(directions, ~paste0(.x, .y)) %>% 
    purrr::map(safe_char2dms) %>% 
    purrr::map(purrr::pluck("result")) %>% 
    purrr::map(as.numeric) %>% 
    unlist() %>% 
    return()
}

# clean ant transform status data in to ordenated factor
statusToFactor <- function(.status){
  tibble( from_status = tolower(.status) )  %>% 
    mutate(
      to_status = case_when(
        str_detect(from_status, "manchas")   ~ "stains",
        str_detect(from_status, "esparsos")  ~ "traces/sparse",
        str_detect(from_status, "observado") ~ "not observed",
        T ~ as.character(NA)
      )
    ) %>% 
    mutate( fct_status = factor(to_status, levels = c("not observed", "traces/sparse", "stains"), ordered = T) ) %>% 
    pull(fct_status) %>% 
    return()
}

# read the raw excel importation
files <- readRDS("./data/oil_leakage_imported_raw_excel.rds")

# put all the excels in one dataframe
# perform a data cleanup
stains_raw <- files %>% 
  # the file form day 2019-11-06 hasn't coordinates
  keep(function(.x){ .x$file.date[1] != "2019-11-06"} ) %>% 
  # for each "excel" data
  map_df(function(.x){
    
    # some excels have two "municipio" columns
    if("municipio_" %in% names(.x)) .x <- select(.x, -municipio)
    
    # clean and normalize colnames
    col.names <- .x %>%
      names() %>% 
      tolower() %>%
      str_replace_all("munic.+pi.*", "municipio") %>% # acento no municipio 
      str_replace_all("name", "localidade") %>% 
      str_replace_all("latitutde","latitude")
    
    # rename the cols and select what we'll use
    .x %>% 
      set_names(col.names) %>% 
      select(file.date, localidade, municipio, 
             estado, data_avist, data_revis,
             latitude, longitude, status) %>% 
      
      # handle portuguese accents
      mutate(
        localidade = iconv(localidade, from="UTF-8", to="LATIN1"),
        municipio  = iconv(municipio, from="UTF-8", to="LATIN1"),
        estado     = iconv(estado, from="UTF-8", to="LATIN1"),
        status     = iconv(status, from="UTF-8", to="LATIN1"),
        data_avist = ymd(as.character(data_avist)), 
        data_revis = ymd(as.character(data_revis))
      ) %>% 
      return()
  })

# we have to translate degree coordinates to decimal
stains_coord <- stains_raw %>%   
  mutate(
    lat.degree = translateCoord(latitude),
    lon.degree = translateCoord(longitude)
  ) %>% 
  # in some of the files, the coordinates are indeed in decimal 
  mutate(
    lat.degree = ifelse(is.na(lat.degree), as.numeric(latitude), lat.degree),
    lon.degree = ifelse(is.na(lon.degree), as.numeric(longitude), lon.degree)
  )

# in some files, the "UF" column has the name of the states 
# and others the code of states, we handle this making a name<->code table
estado_uf <- files %>% 
  tail(1) %>% 
  .[[1]] %>% 
  mutate(
    estado = iconv(estado, from="UTF-8", to="LATIN1")
  ) %>% 
  select(estado, sigla_uf) %>% 
  distinct() %>% 
  arrange(estado)

# join the state name<->code table
stains_clean <- stains_coord %>% 
  left_join(estado_uf, by = "estado") %>% 
  # handle when the info is correct/incorrect
  mutate(sigla_uf = ifelse(is.na(sigla_uf),estado,sigla_uf)) %>% 
  # there is files without state code
  filter(!is.na(sigla_uf)) %>% 
  select(-estado) %>% 
  # rebuild state name
  inner_join(estado_uf, by="sigla_uf") %>% 
  # reselect only interesting cols
  select(file.date, data_avist, data_revis, sigla_uf, estado, municipio, localidade, lat.degree, lon.degree, status)

# transform the "status" info in a factor column
stains <- stains_clean %>% 
  mutate( status = statusToFactor(status) ) %>% 
  filter(complete.cases(.)) %>% 
  # english col names
  set_names(c("file.date", "sighting_date","revision_date","uf","state","county",
              "localidade","lat.degree","lon.degree","status"))

# save it, just in case
saveRDS(stains, "./data/oil_leakage.rds")

```

Agora nós temos todos os dados dos arquivos excels concatenados, limpos e tratados, vamos olhar o formato final.

```{r seeDataset}

# checking the final format
stains %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 8)
```

## `ggmap package`

Com os dados na mão, vamos tentar visualizar a comunidades afetadas pelo vazamento de óleo colocando os dados em um map, há vários frameworks para plotar maps em R, neste post vamos usar o [pacote `ggmap`](https://cran.r-project.org/web/packages/ggmap/index.html), que já utilizamos [anteriormente no passado](https://yetanotheriteration.netlify.com/2018/01/ploting-your-mtb-track-with-r/).

```{r ggmapTest, warning=FALSE, message=FALSE}

library(ggmap)
library(yaml)

# before use ggmap with google maps it's necessary
# read and register a Key for Google Map API
config <- yaml::read_yaml("./config.yml")
register_google(key=config$google_map_key)

stains_on_date <- stains %>%
    filter(status!="not observed", 
           revision_date == ymd(20191001),
           lon.degree <0) 

bbox <- make_bbox(lon = stains_on_date$lon.degree, lat=stains_on_date$lat.degree, f = .1)
gmap <- get_map(location=bbox, source="google", maptype="terrain")

ggmap(gmap) +
  geom_point(data=stains, 
             aes(x=lon.degree, y=lat.degree, color=status), size=2, alpha=.5) +
  scale_color_manual(values=c("black","darkgrey","green")) +
  theme_void() +
  ggplot2::coord_fixed() +
  theme( legend.position = "none")

```

## `gganimation package`


