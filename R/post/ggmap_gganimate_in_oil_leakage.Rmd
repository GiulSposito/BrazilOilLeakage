---
title: "Using ggmap and gganimation to visualize oil spill in Brazil"
output:
  html_document:
    df_print: paged
---

In 2019, a crude oil spill struck more than 2,000 kilometers off the coast of Brazil's Northeast and Southeast, affecting more than 400 beaches in more than 200 different municipalities. The first reports of the spill occurred at the end of August with sightings still spreading later this year. This post explores the sighting records available on the IBAMA website to view the impact of the leak using ggmap and gganimation.

<!--more--> 

In 2019, [a crude oil spill](https://en.wikipedia.org/wiki/2019_Northeast_Brazil_oil_spill) struck more than 2,000 kilometers off the coast of Brazil's Northeast and Southeast, affecting more than 400 beaches in more than 200 different municipalities. The first reports of the spill occurred at the end of August with sightings still spreading later this year. More than a thousand tons of oil have already been collected from the beaches, which is the worst oil leak in Brazil's history and the largest environmental disaster on the Brazilian coast.


In this post we will explore the oil sighting [data available](http://www.ibama.gov.br/manchasdeoleo) on the [IBAMA](http://www.ibama.gov.br) website (the Brazilian Institute of the Environment and Renewable Natural Resources), a state agency from the [Ministry of the Environment](https://www.mma.gov.br/) responsible for implementing federal environmental preservation policies. We'll try to view the oil spill extension and evolution using the `ggmap` and` gganimation` R packages.

```{r setup, echo=FALSE}
# put rnotebook in the same workdir
# knitr::opts_knit$set(root.dir = normalizePath(rprojroot::find_rstudio_root_file())) 
```

## Site and Dataset

IBAMA has made the oil spill data and information available on a [subsection of its site](http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas), keeping a [daily record of status](http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas) and spotting sightings.

Part of the records provided are available in excel format, containing the following information: the name of each spotted location, the county, the date of first sighting, the state, latitude, longitude, date where the location was revisited and oil spill status at the moment.

## Data Scrapping

Although the site offers PDFs and XLXS, and it is possible to explore the table of sightings within PDFs through the [`tabulizer` package](https://cran.r-project.org/web/packages/tabulizer/vignettes/tabulizer.html), in this post we will only explore the contents of files in excel format. The first step to this is to scrap the page that provides the data to extract the links to download the XLSX files, we'll use [`rvest` package](https://www.datacamp.com/community/tutorials/r-web-scraping-rvest).

```{r excelScrapping, eval=FALSE}
library(tidyverse)  # of course
library(rvest)      # to handle html scrapping
library(lubridate)  # handle datetime formats
library(glue)       # easily concat texts
library(knitr)      # markdown output tables
library(kableExtra) # formate markdown tables

# scrap and build url table
base_url <- "http://www.ibama.gov.br"

# page with update table
page <- read_html("http://www.ibama.gov.br/manchasdeoleo-localidades-atingidas")

# get the table
updates <- page %>% 
  html_table() %>% 
  .[[1]]

# get the href links
doc_links <- page %>%
  html_nodes(xpath = "//td/a") %>% 
  html_attr("href")

# put then togheter
doc_list <- updates %>% 
  set_names(c("date","description","items")) %>% 
  mutate( type = str_extract(items, "^[\\w]+") ) %>% 
  mutate( type = ifelse(type=="XLS","XLSX", type)) %>% # one file with "xls" extension
  mutate( link = paste0(base_url, doc_links) ) %>% 
  mutate( date=dmy(date) ) %>%
  as_tibble()

# save it, just in case we want to recover later
saveRDS(doc_list,"./data/oil_leakage_doc_list.rds")

# let's see the links list
doc_list %>% 
  head(10) %>% 
  kable() %>% 
  kableExtra::kable_styling(font_size = 10)

```


```{r readDocList, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)  # of course
library(rvest)      # to handle html scrapping
library(lubridate)  # handle datetime formats
library(glue)       # easily concat texts
library(knitr)      # markdown output tables
library(kableExtra) # formate markdown tables

# reading from cache
doc_list <- readRDS("./data/oil_leakage_doc_list.rds")

# let's see the links list
doc_list %>% 
  head(10) %>% 
  kable() %>% 
  kableExtra::kable_styling(font_size = 10)
```

Now that we have the links in hand, let's download the XLSX files.

```{r excelDownload, eval=FALSE, message=FALSE, warning=FALSE}
# check the pre existence of destination folders
XLSX_DEST_FOLDER <- "./data/oil_leakage_raw"
if(!file.exists("./data")) dir.create("./data")
if(!file.exists(XLSX_DEST_FOLDER)) dir.create(XLSX_DEST_FOLDER)

# download xlsx files
xlsx_filenames <- doc_list %>% 
  filter(type=="XLSX") %>% 
  select(date, link) %>% 
  arrange(date) %>% 
  split(1:nrow(.)) %>% 
  map_chr(function(.x){
    filename <- paste0(XLSX_DEST_FOLDER, "/",as.character(.x$date[1]), ".xlsx")
    download.file(url=.x$link[1], destfile = filename, mode = "wb")
    return(filename)
  })

# save an excel filename index
xlsx_index <- doc_list %>% 
  filter( type=="XLSX" ) %>% 
  arrange(date) %>% 
  mutate( filename = xlsx_filenames )

# save it
saveRDS(xlsx_index, "./data/excel_file_index.rds")

# let's see what we have
xlsx_index %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 9)
```

```{r readFileIndex, echo=FALSE}
xlsx_index <- readRDS("./data/excel_file_index.rds")

# let's see what we have
xlsx_index %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 9)
```

## Import Data

Let's use the [`xlsx` package](http://www.sthda.com/english/wiki/r-xlsx-package-a-quick-start-guide-to-manipulate-excel-files-in-r) to read excel files and import them into a` data.frame`. Let's take a look at one of them.

```{r viewXLSX}
library(xlsx) # import excel files (requires java/rjava)

xlsx_file <- read.xlsx(file = "./data/oil_leakage_raw/2019-12-14.xlsx",sheetIndex = 1)

xlsx_file %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 10)

```

We can see that the file contains: a _geocode_, the name of each location, a _id_ for that spot, the county name, state name and federation unit acronym, the date of the first spill sighting, the revision date (last status position), the current information about the spill state of the locality, latitude and longitude, plus an "hour" and an unknown `cou` column.

To use the files, you will need to resolve encoding, transform status in a factor, and turn degree notations for latitude and longitude into decimal notation.

### Importation 

Importing the files is straightforward, first let's import all the files and then do the data processing.

```{r xlsxImport, eval=FALSE, message=FALSE, warning=FALSE}

# import each xlsx_file
files <- xlsx_index %>%
  split(1:nrow(.)) %>% 
  map(function(.x){
    print(.x$filename)
    read.xlsx(file = .x$filename[1], sheetIndex = 1, colClasses="character", stringsAsFactors=F) %>% 
      as_tibble() %>% 
      mutate(file.date=.x$date[1])
  })

# save it as cache
saveRDS(files, "./data/oil_leakage_imported_raw_excel.rds")

```

### Data Clean-up

```{r dataCleanup, eval=FALSE, message=FALSE, warning=FALSE}

# auxiliary function to translate the degree coordinates to decimal coordinates
translateCoord <- function(.coords){
  
  # extract orientation (N,S,W,E)
  directions <- str_extract(.coords,"\\w$")
  
  # split the numbers and rebuilds as xxDyy'zz"
  numbers <- .coords %>% 
    str_extract_all("(\\d\\.*)+") %>% 
    purrr::map(~paste0(.x[1],"d",.x[2],"'",.x[3],"\""))
  
  # modify char2dms to a safity call
  safe_char2dms <- safely(sp::char2dms, otherwise = NA)
  
  # readd orientation(NSWE) and converts to decimal
  numbers %>% 
    purrr::map2(directions, ~paste0(.x, .y)) %>% 
    purrr::map(safe_char2dms) %>% 
    purrr::map(purrr::pluck("result")) %>% 
    purrr::map(as.numeric) %>% 
    unlist() %>% 
    return()
}

# clean ant transform status data in to ordenated factor
statusToFactor <- function(.status){
  tibble( from_status = tolower(.status) )  %>% 
    mutate(
      to_status = case_when(
        str_detect(from_status, "manchas")   ~ "stains",
        str_detect(from_status, "esparsos")  ~ "traces/sparse",
        str_detect(from_status, "observado") ~ "not observed",
        T ~ as.character(NA)
      )
    ) %>% 
    mutate( fct_status = factor(to_status, levels = c("not observed", "traces/sparse", "stains"), ordered = T) ) %>% 
    pull(fct_status) %>% 
    return()
}

# read the raw excel importation
files <- readRDS("./data/oil_leakage_imported_raw_excel.rds")

# put all the excels in one dataframe
# perform a data cleanup
stains_raw <- files %>% 
  # the file form day 2019-11-06 hasn't coordinates
  keep(function(.x){ .x$file.date[1] != "2019-11-06"} ) %>% 
  # for each "excel" data
  map_df(function(.x){
    
    # some excels have two "municipio" columns
    if("municipio_" %in% names(.x)) .x <- select(.x, -municipio)
    
    # clean and normalize colnames
    col.names <- .x %>%
      names() %>% 
      tolower() %>%
      str_replace_all("munic.+pi.*", "municipio") %>% # acento no municipio 
      str_replace_all("name", "localidade") %>% 
      str_replace_all("latitutde","latitude")
    
    # rename the cols and select what we'll use
    .x %>% 
      set_names(col.names) %>% 
      select(file.date, localidade, municipio, 
             estado, data_avist, data_revis,
             latitude, longitude, status) %>% 
      
      # handle portuguese accents
      mutate(
        localidade = iconv(localidade, from="UTF-8", to="LATIN1"),
        municipio  = iconv(municipio, from="UTF-8", to="LATIN1"),
        estado     = iconv(estado, from="UTF-8", to="LATIN1"),
        status     = iconv(status, from="UTF-8", to="LATIN1"),
        data_avist = ymd(as.character(data_avist)), 
        data_revis = ymd(as.character(data_revis))
      ) %>% 
      return()
  })

# we have to translate degree coordinates to decimal
stains_coord <- stains_raw %>%   
  mutate(
    lat.degree = translateCoord(latitude),
    lon.degree = translateCoord(longitude)
  ) %>% 
  # in some of the files, the coordinates are indeed in decimal 
  mutate(
    lat.degree = ifelse(is.na(lat.degree), as.numeric(latitude), lat.degree),
    lon.degree = ifelse(is.na(lon.degree), as.numeric(longitude), lon.degree)
  )

# in some files, the "UF" column has the name of the states 
# and others the code of states, we handle this making a name<->code table
estado_uf <- files %>% 
  tail(1) %>% 
  .[[1]] %>% 
  mutate(
    estado = iconv(estado, from="UTF-8", to="LATIN1")
  ) %>% 
  select(estado, sigla_uf) %>% 
  distinct() %>% 
  arrange(estado)

# join the state name<->code table
stains_clean <- stains_coord %>% 
  left_join(estado_uf, by = "estado") %>% 
  # handle when the info is correct/incorrect
  mutate(sigla_uf = ifelse(is.na(sigla_uf),estado,sigla_uf)) %>% 
  # there is files without state code
  filter(!is.na(sigla_uf)) %>% 
  select(-estado) %>% 
  # rebuild state name
  inner_join(estado_uf, by="sigla_uf") %>% 
  # reselect only interesting cols
  select(file.date, data_avist, data_revis, sigla_uf, estado, municipio, localidade, lat.degree, lon.degree, status)

# transform the "status" info in a factor column
stains <- stains_clean %>% 
  mutate( status = statusToFactor(status) ) %>% 
  filter(complete.cases(.)) %>% 
  # english col names
  set_names(c("file.date", "sighting_date","revision_date","uf","state","county",
              "local","lat.degree","lon.degree","status")) %>% 
  # last clean-up
  filter(
    lon.degree < 0, # anything greater than that is a error
    revision_date < ymd(20200101), # anything greater than that is a error
    sighting_date < ymd(20200101), # anything greater than that is a error
    complete.cases(.) # missing data is not relevant
  )

# save it, just in case
saveRDS(stains, "./data/oil_leakage.rds")

```

Now we have all the data from the excel files concatenated, cleaned and treated, let's look at the final data format.

```{r seeDataset}

stains <- readRDS("./data/oil_leakage.rds")

# checking the final format
stains %>% 
  head(10) %>% 
  kable() %>% 
  kable_styling(font_size = 8)
```

## `ggmap package`

With the data in hand, we' wi'll try to visualize the communities affected by the oil spill by placing the data in a map. There are several frameworks for plotting maps in R, in this post we will use the [`ggmap` package](https://www.littlemissdata.com/blog/maps), which we have used [previously in the past](https://yetanotheriteration.netlify.com/2018/01/ploting-your-mtb-track-with-r/).

Before to use ggmap, you'll need to get the Google Map API Key and register it, but the procedure is pretty straightforward, [just follow the instructions](https://developers.google.com/maps/documentation/embed/get-api-key). In my code, I always store the keys in a [`yalm` file](https://en.wikipedia.org/wiki/YAML), to avoid accidentaly to publish in the GitHub, [I also commented this strategy in a older post](https://yetanotheriteration.netlify.com/2019/03/comparing-fitbit-and-polar-h7-heart-rate-data/). 


```{r ggmapTest, warning=FALSE, message=FALSE}
# ggmap
library(ggmap)
library(yaml) # used to not version the google's map key

# before use ggmap with google maps it's necessary
# read and register a Key for Google Map API
config <- yaml::read_yaml("./config.yml")
register_google(key=config$google_map_key)

# get the map area
bbox <- make_bbox(lon = stains$lon.degree, lat=stains$lat.degree, f = .1)
gmap <- get_map(location=bbox, source="google", maptype="terrain")

# plot the data of observations on revision date 2019-11-10
# of location with stains or sparse residuos
ggmap(gmap) +
  geom_point(data=filter(stains, status!="not observed", revision_date==ymd(20191110)),
             aes(x=lon.degree, y=lat.degree, color=status), size=2, alpha=.5) +
  scale_color_manual(values=c("orange","red","green")) +
  theme_void() +
  ggplot2::coord_fixed() 
```

## `gganimate` package

Now the last step is to animate the map in the time dimension, we'll do this using the great [`gganimate` package](https://gganimate.com/articles/gganimate.html), adding a transition function in the `ggplot2/ggmap` stack.

```{r animation, warning=FALSE, message=FALSE}
library(gganimate)
bbox <- make_bbox(lon = stains$lon.degree, lat=stains$lat.degree, f = .1)
gmap <- get_map(location=bbox, source="google", maptype="terrain")

p <- ggmap(gmap) +
  geom_point(data=filter(stains, status!="not observed"),
             aes(x=lon.degree, y=lat.degree,
                 color=status, group=county),
             size=3, alpha=.5) +
  scale_color_manual(values=c("orange","red")) +
  theme_void()

anim <- p  +
  theme( legend.position = "bottom" ) +
  transition_time(revision_date) +
  labs(title="Oil Sighting on Brazil Coast",
       subtitle = "Date:  {frame_time} - source: IBAMA") +
  shadow_wake(wake_length = .15)

anim

```

## Visualizing the spill evolution in one plot

The animated map view gives us an idea of the size of the ecological impact of the oil spill off the coast of Brazil, but the animation does not allow you to see the full evolution history at the same time,  to allow us to gauge how the spill is progressing. 

To do this trick, let's plot the information as a heatmap, where each location will be on the Y axis, indexed by its latitude, and on the X axis we'll use the date information.


```{r tileChart}

# let's organize the "county/locality" by latitude
stains %>% 
  select(local, lat.degree, revision_date, status) %>% 
  group_by(local) %>% 
  # calculates a unique latitude for local
  mutate( avg.lat = mean(lat.degree) ) %>% 
  ungroup() %>% 
  # put the locals in latitude order
  mutate( local = fct_reorder(local, avg.lat) ) %>% 
  select( local, revision_date, status ) %>% 
  distinct() %>% 
  # plot as a tile chart (local x date)
  arrange( local, revision_date ) %>% 
  ggplot(aes(x=revision_date, y=local)) +
  geom_tile(aes(fill=status)) +
  scale_fill_manual(values = c("lightgreen", "orange", "red")) +
  theme_minimal() +
  theme( axis.text.y = element_blank() ) +
  labs(title="Oil Sighting by Latitude and Date", x="date", y="local/latitude")

```

In this graph, we can visualize how the the oil spill are progressing to the south, following the maritime currents, over the months of October and November, reaching the most extreme point a few days,  before December.

The intermittent nature of the chart shows that information registring is not done daily in all locations, and some communities has more status updates than others we should handle this caracteristic to correctly plot the information, but we'll leave this to other post.